{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b5bcb37",
   "metadata": {},
   "source": [
    "## 1. Problem Formulation\n",
    "\n",
    "### 1.1 Binary Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a278f6",
   "metadata": {},
   "source": [
    "**Objective**: Learn a function that maps nutritional feature vectors to binary fitness labels.\n",
    "\n",
    "$$f: \\mathbb{R}^d \\to \\{0, 1\\}$$\n",
    "\n",
    "**Input Space**: $\\mathcal{X} = \\mathbb{R}^d$, where $d = 9$ (after feature selection)\n",
    "\n",
    "**Output Space**: $\\mathcal{Y} = \\{0, 1\\}$, where:\n",
    "- $y = 1$: Food is \"fit\" (nutritionally favorable)\n",
    "- $y = 0$: Food is \"unfit\" (nutritionally unfavorable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcd88da",
   "metadata": {},
   "source": [
    "**Training Data**:\n",
    "\n",
    "$$\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n}$$\n",
    "\n",
    "where $n = 164$ training samples\n",
    "\n",
    "**Test Data**:\n",
    "\n",
    "$$\\mathcal{D}_{\\text{test}} = \\{(\\mathbf{x}_j, y_j)\\}_{j=1}^{m}$$\n",
    "\n",
    "where $m = 41$ test samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb588a52",
   "metadata": {},
   "source": [
    "**Goal**: Minimize expected 0-1 loss over the data distribution:\n",
    "\n",
    "$$f^* = \\arg\\min_{f \\in \\mathcal{F}} \\mathbb{E}_{(\\mathbf{x}, y) \\sim P}[\\mathbb{1}(f(\\mathbf{x}) \\neq y)]$$\n",
    "\n",
    "This objective seeks the optimal classifier that minimizes prediction errors on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b816f448",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing Pipeline\n",
    "\n",
    "### 2.1 Missing Value Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17635f3a",
   "metadata": {},
   "source": [
    "**Input**: Raw feature matrix $\\mathbf{X}_{\\text{raw}} \\in \\mathbb{R}^{n \\times d_{\\text{raw}}}$ with missing entries (NaN)\n",
    "\n",
    "**Transformation**: Median imputation operator $\\phi_{\\text{impute}}: \\mathbb{R}^{n \\times d_{\\text{raw}}} \\to \\mathbb{R}^{n \\times d_{\\text{raw}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d3add",
   "metadata": {},
   "source": [
    "**Definition**: For each feature column $\\mathbf{x}_{\\cdot, j}$:\n",
    "\n",
    "$$\\phi_{\\text{impute}}(\\mathbf{x}_{\\cdot, j}) = \\begin{cases}\n",
    "x_{i,j} & \\text{if } x_{i,j} \\neq \\text{NaN} \\\\\n",
    "\\tilde{x}_j & \\text{if } x_{i,j} = \\text{NaN}\n",
    "\\end{cases}$$\n",
    "\n",
    "where $\\tilde{x}_j = \\text{median}(\\{x_{i,j} : x_{i,j} \\neq \\text{NaN}\\})$\n",
    "\n",
    "**Explanation**: Missing values are replaced with the median of observed values for that feature. This provides robustness against outliers and maintains the distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b00dec3",
   "metadata": {},
   "source": [
    "### 2.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb92e2c",
   "metadata": {},
   "source": [
    "**Input**: Imputed features $\\mathbf{X}_{\\text{imputed}} \\in \\mathbb{R}^{n \\times 8}$\n",
    "\n",
    "**Transformation**: Feature engineering operator $\\phi_{\\text{engineer}}: \\mathbb{R}^{8} \\to \\mathbb{R}^{13}$\n",
    "\n",
    "Five engineered features are created to capture domain-specific nutritional relationships:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2313695c",
   "metadata": {},
   "source": [
    "**1. Nutrient Density**:\n",
    "\n",
    "$$x_{\\text{nutrient\\_density}} = \\frac{x_{\\text{protein}}}{x_{\\text{calories}} + 1}$$\n",
    "\n",
    "Measures protein content relative to caloric density. Higher values indicate more protein per calorie, which is favorable for fitness.\n",
    "\n",
    "**2. Protein Ratio**:\n",
    "\n",
    "$$x_{\\text{protein\\_ratio}} = \\frac{x_{\\text{protein}}}{x_{\\text{carbs}} + x_{\\text{fat}} + 1}$$\n",
    "\n",
    "Quantifies the proportion of protein relative to other macronutrients. High protein-to-macronutrient ratios are associated with muscle maintenance and satiety."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dd06fe",
   "metadata": {},
   "source": [
    "**3. Carb-Fat Ratio**:\n",
    "\n",
    "$$x_{\\text{carb\\_fat\\_ratio}} = \\frac{x_{\\text{carbs}}}{x_{\\text{fat}} + 1}$$\n",
    "\n",
    "Captures the balance between carbohydrates and fats. Note: This feature was ultimately excluded during feature selection.\n",
    "\n",
    "**4. Energy Density**:\n",
    "\n",
    "$$x_{\\text{energy\\_density}} = \\frac{x_{\\text{calories}}}{100}$$\n",
    "\n",
    "Normalizes calories to a standard portion size (per 100g), enabling comparison across different food quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654fd488",
   "metadata": {},
   "source": [
    "**5. Micronutrient Score**:\n",
    "\n",
    "$$x_{\\text{micronutrient\\_score}} = x_{\\text{iron}} + x_{\\text{vitamin\\_c}}$$\n",
    "\n",
    "Aggregates key micronutrient content. Iron and Vitamin C are essential nutrients that signal overall nutritional quality.\n",
    "\n",
    "**Note**: The $+1$ terms in denominators prevent division by zero and provide numerical stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a756125",
   "metadata": {},
   "source": [
    "**Augmented Feature Vector**:\n",
    "\n",
    "$$\\mathbf{x}_{\\text{engineered}} = [\\mathbf{x}_{\\text{raw}}; \\mathbf{x}_{\\text{derived}}] \\in \\mathbb{R}^{13}$$\n",
    "\n",
    "where $[\\cdot; \\cdot]$ denotes vector concatenation. The original 8 features are combined with 5 derived features to create a 13-dimensional feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae399b",
   "metadata": {},
   "source": [
    "### 2.3 Categorical Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa7320",
   "metadata": {},
   "source": [
    "**Input**: Engineered features with categorical variable $c \\in \\mathcal{C}$ (food category)\n",
    "\n",
    "where $|\\mathcal{C}| = 53$ unique categories\n",
    "\n",
    "**Transformation**: One-hot encoding operator $\\phi_{\\text{encode}}: \\mathcal{C} \\to \\{0, 1\\}^{53}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3337453e",
   "metadata": {},
   "source": [
    "**Definition**:\n",
    "\n",
    "$$\\phi_{\\text{encode}}(c) = [e_1, e_2, \\ldots, e_{53}], \\quad e_k = \\begin{cases}\n",
    "1 & \\text{if } c = \\text{category}_k \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Each food category is converted into a binary indicator vector. Only one element is 1 (the category of the food), while all others are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09753bb3",
   "metadata": {},
   "source": [
    "**Augmented Feature Vector** (including dietary flags):\n",
    "\n",
    "$$\\mathbf{x}_{\\text{encoded}} = [\\mathbf{x}_{\\text{engineered}}; \\phi_{\\text{encode}}(c); \\mathbf{x}_{\\text{dietary}}] \\in \\mathbb{R}^{69}$$\n",
    "\n",
    "where $\\mathbf{x}_{\\text{dietary}} = [x_{\\text{glutenfree}}, x_{\\text{nutfree}}, x_{\\text{vegan}}] \\in \\{0, 1\\}^3$\n",
    "\n",
    "The final encoded feature space has 69 dimensions: 13 nutritional features + 53 category indicators + 3 dietary flags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e341286",
   "metadata": {},
   "source": [
    "### 2.4 Target Label Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a186e1",
   "metadata": {},
   "source": [
    "**Input**: Raw nutritional features\n",
    "\n",
    "$$\\mathbf{x} = [x_{\\text{protein}}, x_{\\text{vitamin\\_c}}, x_{\\text{iron}}, x_{\\text{calories}}, x_{\\text{fat}}]$$\n",
    "\n",
    "**Transformation**: Adaptive labeling function $\\phi_{\\text{label}}: \\mathbb{R}^5 \\to \\{0, 1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38919646",
   "metadata": {},
   "source": [
    "**Fitness Score**:\n",
    "\n",
    "$$S_{\\text{fitness}}(\\mathbf{x}) = 2 \\cdot x_{\\text{protein}} + 1.5 \\cdot x_{\\text{vitamin\\_c}} + 1.5 \\cdot x_{\\text{iron}} - 0.5 \\cdot x_{\\text{calories}} - 0.8 \\cdot x_{\\text{fat}}$$\n",
    "\n",
    "This weighted score favors:\n",
    "- High protein content (coefficient 2.0)\n",
    "- High micronutrients: Vitamin C and Iron (coefficients 1.5 each)\n",
    "- Low calories and fat (negative coefficients -0.5 and -0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f555aff",
   "metadata": {},
   "source": [
    "**Percentile Threshold** (70th percentile):\n",
    "\n",
    "$$\\tau = Q_{0.70}(\\{S_{\\text{fitness}}(\\mathbf{x}_i)\\}_{i=1}^{n})$$\n",
    "\n",
    "The threshold is determined adaptively from the data distribution, ensuring balanced class proportions.\n",
    "\n",
    "**Binary Label**:\n",
    "\n",
    "$$y = \\phi_{\\text{label}}(\\mathbf{x}) = \\begin{cases}\n",
    "1 & \\text{if } S_{\\text{fitness}}(\\mathbf{x}) \\geq \\tau \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Foods scoring above the 70th percentile are labeled \"fit\" (1), others as \"unfit\" (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0023f",
   "metadata": {},
   "source": [
    "## 3. Feature Selection\n",
    "\n",
    "### 3.1 Chi-Squared Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db98e8c1",
   "metadata": {},
   "source": [
    "**Input**: Encoded feature matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times 69}$, target labels $\\mathbf{y} \\in \\{0, 1\\}^n$\n",
    "\n",
    "**Objective**: Select $k = 9$ features with highest chi-squared scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e5fa2",
   "metadata": {},
   "source": [
    "**Chi-Squared Statistic** (for feature $j$):\n",
    "\n",
    "$$\\chi^2_j = \\sum_{i=1}^{n} \\sum_{c=0}^{1} \\frac{(O_{ic}^{(j)} - E_{ic}^{(j)})^2}{E_{ic}^{(j)}}$$\n",
    "\n",
    "where:\n",
    "- $O_{ic}^{(j)}$ = Observed frequency of feature bin $i$ in class $c$\n",
    "- $E_{ic}^{(j)}$ = Expected frequency under independence: $E_{ic}^{(j)} = \\frac{n_i \\cdot n_c}{n}$\n",
    "\n",
    "The chi-squared test measures the dependence between each feature and the target variable. Higher scores indicate stronger statistical association."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2209616e",
   "metadata": {},
   "source": [
    "**Feature Selection Operator** $\\phi_{\\text{select}}: \\mathbb{R}^{69} \\to \\mathbb{R}^{9}$:\n",
    "\n",
    "$$\\phi_{\\text{select}}(\\mathbf{x}) = [\\mathbf{x}_{j_1}, \\mathbf{x}_{j_2}, \\ldots, \\mathbf{x}_{j_9}]$$\n",
    "\n",
    "where $j_1, j_2, \\ldots, j_9$ are indices of features with top 9 chi-squared scores:\n",
    "\n",
    "$$\\{j_1, \\ldots, j_9\\} = \\arg\\max_{|S|=9, S \\subseteq \\{1,\\ldots,69\\}} \\sum_{j \\in S} \\chi^2_j$$\n",
    "\n",
    "This greedy selection retains only the most informative features for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a7e5d",
   "metadata": {},
   "source": [
    "**Selected Features** (ordered by $\\chi^2$ score):\n",
    "\n",
    "$$\\mathbf{x}_{\\text{selected}} = [x_{\\text{vitamin\\_c}}, x_{\\text{nutrient\\_density}}, x_{\\text{micronutrient\\_score}}, x_{\\text{calories}}, x_{\\text{category\\_beverages}}, x_{\\text{fat}}, x_{\\text{category\\_chips}}, x_{\\text{protein\\_ratio}}, x_{\\text{energy\\_density}}]$$\n",
    "\n",
    "The final 9 features include:\n",
    "- 3 raw nutritional features (vitamin_c, calories, fat)\n",
    "- 4 engineered features (nutrient_density, micronutrient_score, protein_ratio, energy_density)\n",
    "- 2 category indicators (beverages, chips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c0792",
   "metadata": {},
   "source": [
    "## 4. Random Forest Classifier\n",
    "\n",
    "### 4.1 Decision Tree Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1735189",
   "metadata": {},
   "source": [
    "**Input**: Training data $\\mathcal{D}_t = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n_t}$ (bootstrap sample)\n",
    "\n",
    "**Tree Structure**: Binary tree $h_t: \\mathbb{R}^9 \\to \\{0, 1\\}$\n",
    "\n",
    "Each tree recursively partitions the feature space via binary splits to create homogeneous leaf nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7c1b7",
   "metadata": {},
   "source": [
    "**Gini Impurity** (for node $N$):\n",
    "\n",
    "$$G(N) = 1 - \\sum_{c=0}^{1} p_c^2$$\n",
    "\n",
    "where $p_c = \\frac{1}{|\\mathcal{S}_N|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{S}_N} \\mathbb{1}(y = c)$\n",
    "\n",
    "For binary classification:\n",
    "\n",
    "$$G(N) = 2p_0 p_1 = 2 p_1 (1 - p_1)$$\n",
    "\n",
    "Gini impurity measures the heterogeneity of labels in a node. Pure nodes (all samples same class) have $G(N) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e390e9f9",
   "metadata": {},
   "source": [
    "**Optimal Split** (at node $N$):\n",
    "\n",
    "$$(f^*, \\tau^*) = \\arg\\max_{f \\in F_N, \\tau \\in \\mathbb{R}} \\Delta G(N, f, \\tau)$$\n",
    "\n",
    "where:\n",
    "- $F_N$: Random subset of 3 features (out of 9) - feature bagging for decorrelation\n",
    "- $\\Delta G(N, f, \\tau)$: Gini gain from split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45535461",
   "metadata": {},
   "source": [
    "**Gini Gain**:\n",
    "\n",
    "$$\\Delta G(N, f, \\tau) = G(N) - \\left( \\frac{|\\mathcal{S}_{N_L}|}{|\\mathcal{S}_N|} G(N_L) + \\frac{|\\mathcal{S}_{N_R}|}{|\\mathcal{S}_N|} G(N_R) \\right)$$\n",
    "\n",
    "where:\n",
    "- $N_L$: Left child (samples with $x_f \\leq \\tau$)\n",
    "- $N_R$: Right child (samples with $x_f > \\tau$)\n",
    "\n",
    "The split maximizes the reduction in impurity weighted by the proportion of samples sent to each child."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20946f2",
   "metadata": {},
   "source": [
    "**Stopping Criteria**: Tree construction terminates at node $N$ if any condition is met:\n",
    "\n",
    "1. **Max Depth**: $\\text{depth}(N) = 5$ (prevents overfitting)\n",
    "2. **Min Samples Split**: $|\\mathcal{S}_N| < 5$ (insufficient samples to split)\n",
    "3. **Min Samples Leaf**: Would create leaf with $<3$ samples (ensures statistical reliability)\n",
    "4. **Pure Node**: $G(N) = 0$ (all samples same class)\n",
    "\n",
    "These hyperparameters were optimized via GridSearchCV for the small dataset (n=164)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb05f078",
   "metadata": {},
   "source": [
    "**Leaf Prediction**:\n",
    "\n",
    "$$\\hat{y}_N = \\arg\\max_{c \\in \\{0, 1\\}} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{S}_N} \\mathbb{1}(y = c)$$\n",
    "\n",
    "The predicted class is the majority class among samples reaching the leaf.\n",
    "\n",
    "**Leaf Probability Estimate**:\n",
    "\n",
    "$$P_N(y = 1) = \\frac{1}{|\\mathcal{S}_N|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{S}_N} \\mathbb{1}(y = 1)$$\n",
    "\n",
    "The proportion of positive class samples provides a probability estimate for classification confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73682019",
   "metadata": {},
   "source": [
    "### 4.2 Bootstrap Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b435ab6",
   "metadata": {},
   "source": [
    "**Training Set**: $\\mathcal{D}_{\\text{train}} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n}$, $n = 164$\n",
    "\n",
    "**Bootstrap Sample for Tree $t$**:\n",
    "\n",
    "$$\\mathcal{D}_t = \\{(\\mathbf{x}_{i_1}, y_{i_1}), (\\mathbf{x}_{i_2}, y_{i_2}), \\ldots, (\\mathbf{x}_{i_n}, y_{i_n})\\}$$\n",
    "\n",
    "where $i_j \\sim \\text{Discrete Uniform}(\\{1, \\ldots, n\\})$ (sampling with replacement)\n",
    "\n",
    "Each tree sees a different random sample of the training data, promoting diversity in the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90beda1f",
   "metadata": {},
   "source": [
    "**Expected Unique Samples**:\n",
    "\n",
    "$$\\mathbb{E}[|\\{\\text{unique samples in } \\mathcal{D}_t\\}|] \\approx n \\left(1 - \\frac{1}{e}\\right) \\approx 0.632n \\approx 104 \\text{ samples}$$\n",
    "\n",
    "On average, each bootstrap sample contains about 63.2% unique samples, with some samples appearing multiple times and others not at all. This creates natural train-validation splits within each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680916cf",
   "metadata": {},
   "source": [
    "### 4.3 Ensemble Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baef142e",
   "metadata": {},
   "source": [
    "**Forest Definition**: Collection of $T = 50$ trees $\\{h_1, h_2, \\ldots, h_T\\}$\n",
    "\n",
    "**Class Probability Estimation**:\n",
    "\n",
    "$$P_{\\text{RF}}(y = 1 \\mid \\mathbf{x}) = \\frac{1}{T} \\sum_{t=1}^{T} P_t(y = 1 \\mid \\mathbf{x})$$\n",
    "\n",
    "where $P_t(y = 1 \\mid \\mathbf{x})$ is the probability estimate from tree $t$ (leaf node probability).\n",
    "\n",
    "The ensemble averages probability estimates across all trees to produce a smooth, calibrated prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc84967",
   "metadata": {},
   "source": [
    "**Classification Rule**:\n",
    "\n",
    "$$\\hat{y}_{\\text{RF}}(\\mathbf{x}) = \\begin{cases}\n",
    "1 & \\text{if } P_{\\text{RF}}(y = 1 \\mid \\mathbf{x}) \\geq 0.5 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Alternative (Majority Vote)**:\n",
    "\n",
    "$$\\hat{y}_{\\text{RF}}(\\mathbf{x}) = \\arg\\max_{c \\in \\{0, 1\\}} \\sum_{t=1}^{T} \\mathbb{1}(h_t(\\mathbf{x}) = c)$$\n",
    "\n",
    "Both formulations are equivalent for binary classification with threshold 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b785cbd",
   "metadata": {},
   "source": [
    "### 4.4 Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c411b693",
   "metadata": {},
   "source": [
    "**Gini Importance for Feature $f$**:\n",
    "\n",
    "$$I(f) = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{N \\in \\text{Nodes}_t(f)} \\frac{|\\mathcal{S}_N|}{n} \\cdot \\Delta G(N, f, \\tau_N)$$\n",
    "\n",
    "where $\\text{Nodes}_t(f)$ is the set of nodes in tree $t$ that split on feature $f$.\n",
    "\n",
    "Feature importance aggregates the total Gini gain contributed by each feature across all splits in all trees, weighted by the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01eae1f",
   "metadata": {},
   "source": [
    "**Normalized Importance**:\n",
    "\n",
    "$$I_{\\text{norm}}(f) = \\frac{I(f)}{\\sum_{f'=1}^{9} I(f')}$$\n",
    "\n",
    "Normalization ensures importances sum to 1, allowing interpretation as relative contributions.\n",
    "\n",
    "**NutriSolve Feature Importances**:\n",
    "\n",
    "$$\\mathbf{I} = \\begin{bmatrix}\n",
    "I(\\text{vitamin\\_c}) \\\\\n",
    "I(\\text{micronutrient\\_score}) \\\\\n",
    "I(\\text{nutrient\\_density}) \\\\\n",
    "\\vdots \\\\\n",
    "I(\\text{category\\_Potato chips})\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.386 \\\\\n",
    "0.189 \\\\\n",
    "0.161 \\\\\n",
    "\\vdots \\\\\n",
    "0.009\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Vitamin C is the most important feature (38.6%), followed by micronutrient score (18.9%) and nutrient density (16.1%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0f673",
   "metadata": {},
   "source": [
    "## 5. Loss Function and Optimization\n",
    "\n",
    "### 5.1 Zero-One Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37357b",
   "metadata": {},
   "source": [
    "**Definition**:\n",
    "\n",
    "$$L_{0-1}(y, \\hat{y}) = \\mathbb{1}(y \\neq \\hat{y}) = \\begin{cases}\n",
    "0 & \\text{if } y = \\hat{y} \\\\\n",
    "1 & \\text{if } y \\neq \\hat{y}\n",
    "\\end{cases}$$\n",
    "\n",
    "The 0-1 loss penalizes misclassifications uniformly, regardless of confidence. It directly corresponds to classification error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcaaf48",
   "metadata": {},
   "source": [
    "**Empirical Risk** (training error):\n",
    "\n",
    "$$R_{\\text{emp}}(h) = \\frac{1}{n} \\sum_{i=1}^{n} L_{0-1}(y_i, h(\\mathbf{x}_i))$$\n",
    "\n",
    "**Expected Risk** (generalization error):\n",
    "\n",
    "$$R(h) = \\mathbb{E}_{(\\mathbf{x}, y) \\sim P}[L_{0-1}(y, h(\\mathbf{x}))]$$\n",
    "\n",
    "The goal is to minimize expected risk, which is estimated by test error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baba6a1a",
   "metadata": {},
   "source": [
    "**NutriSolve Performance**:\n",
    "\n",
    "- Training Error: $R_{\\text{emp}}(h_{\\text{RF}}) = 1 - 0.9390 = 0.0610$ (6.10%)\n",
    "- Test Error: $\\hat{R}(h_{\\text{RF}}) = 1 - 0.9512 = 0.0488$ (4.88%)\n",
    "\n",
    "The model generalizes well (test error < training error), indicating no overfitting despite the small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78be4341",
   "metadata": {},
   "source": [
    "### 5.2 Gini Impurity Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a410019",
   "metadata": {},
   "source": [
    "Each tree optimizes local Gini impurity at each split:\n",
    "\n",
    "$$(f^*, \\tau^*) = \\arg\\min_{f, \\tau} \\left( \\frac{|\\mathcal{S}_{N_L}|}{|\\mathcal{S}_N|} G(N_L) + \\frac{|\\mathcal{S}_{N_R}|}{|\\mathcal{S}_N|} G(N_R) \\right)$$\n",
    "\n",
    "**Equivalently** (maximizing Gini gain):\n",
    "\n",
    "$$(f^*, \\tau^*) = \\arg\\max_{f, \\tau} \\Delta G(N, f, \\tau)$$\n",
    "\n",
    "The greedy split selection ensures each partition increases purity, leading to accurate leaf predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b053b3",
   "metadata": {},
   "source": [
    "### 5.3 Ensemble Risk Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f177780",
   "metadata": {},
   "source": [
    "The expected test error of the ensemble can be decomposed:\n",
    "\n",
    "$$\\mathbb{E}[\\text{Error}_{\\text{ensemble}}] = \\text{Bias}^2 + \\text{Variance} + \\sigma^2_{\\epsilon}$$\n",
    "\n",
    "**Bias**:\n",
    "\n",
    "$$\\text{Bias}(h_{\\text{RF}}) = \\mathbb{E}_{\\mathcal{D}}[\\hat{y}_{\\text{RF}}(\\mathbf{x})] - y^*(\\mathbf{x})$$\n",
    "\n",
    "**Variance**:\n",
    "\n",
    "$$\\text{Variance}(h_{\\text{RF}}) = \\mathbb{E}_{\\mathcal{D}}[(\\hat{y}_{\\text{RF}}(\\mathbf{x}) - \\mathbb{E}_{\\mathcal{D}}[\\hat{y}_{\\text{RF}}(\\mathbf{x})])^2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5d6c1f",
   "metadata": {},
   "source": [
    "**Bagging Variance Reduction**: For ensemble of $T$ independent trees with variance $\\sigma^2$:\n",
    "\n",
    "$$\\text{Var}(\\bar{h}_{\\text{ensemble}}) = \\frac{\\sigma^2}{T}$$\n",
    "\n",
    "With correlation $\\rho$ between trees:\n",
    "\n",
    "$$\\text{Var}(\\bar{h}_{\\text{ensemble}}) = \\rho \\sigma^2 + \\frac{1-\\rho}{T} \\sigma^2$$\n",
    "\n",
    "**NutriSolve**: $T = 50$, $\\rho \\approx 0.6$ (estimated) â†’ Variance reduced by factor of ~8.\n",
    "\n",
    "Bootstrap sampling and feature bagging decorrelate trees, reducing variance while maintaining low bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbdfcc6",
   "metadata": {},
   "source": [
    "## 6. Complete Pipeline: Composite Function\n",
    "\n",
    "### 6.1 End-to-End Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98d73f",
   "metadata": {},
   "source": [
    "The complete NutriSolve ML pipeline is a composition of preprocessing and prediction functions:\n",
    "\n",
    "$$\\hat{y} = f_{\\text{pipeline}}(\\mathbf{x}_{\\text{raw}}) = h_{\\text{RF}} \\circ \\phi_{\\text{select}} \\circ \\phi_{\\text{encode}} \\circ \\phi_{\\text{engineer}} \\circ \\phi_{\\text{impute}}(\\mathbf{x}_{\\text{raw}})$$\n",
    "\n",
    "**Step-by-Step**:\n",
    "1. **Imputation**: $\\mathbf{x}_1 = \\phi_{\\text{impute}}(\\mathbf{x}_{\\text{raw}}) \\in \\mathbb{R}^8$\n",
    "2. **Engineering**: $\\mathbf{x}_2 = \\phi_{\\text{engineer}}(\\mathbf{x}_1) \\in \\mathbb{R}^{13}$\n",
    "3. **Encoding**: $\\mathbf{x}_3 = \\phi_{\\text{encode}}(\\mathbf{x}_2) \\in \\mathbb{R}^{69}$\n",
    "4. **Selection**: $\\mathbf{x}_4 = \\phi_{\\text{select}}(\\mathbf{x}_3) \\in \\mathbb{R}^{9}$\n",
    "5. **Classification**: $\\hat{y} = h_{\\text{RF}}(\\mathbf{x}_4) \\in \\{0, 1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e78d0a5",
   "metadata": {},
   "source": [
    "### 6.2 Dimensionality Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da30c027",
   "metadata": {},
   "source": [
    "$$\\mathbb{R}^8 \\xrightarrow{\\phi_{\\text{impute}}} \\mathbb{R}^8 \\xrightarrow{\\phi_{\\text{engineer}}} \\mathbb{R}^{13} \\xrightarrow{\\phi_{\\text{encode}}} \\mathbb{R}^{69} \\xrightarrow{\\phi_{\\text{select}}} \\mathbb{R}^9 \\xrightarrow{h_{\\text{RF}}} \\{0, 1\\}$$\n",
    "\n",
    "The feature space expands from 8 to 69 dimensions through engineering and encoding, then contracts to 9 dimensions via feature selection before final classification. This expansion-contraction strategy captures complex interactions while maintaining model interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a674b",
   "metadata": {},
   "source": [
    "## 7. Computational Complexity\n",
    "\n",
    "### 7.1 Training Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325e30c1",
   "metadata": {},
   "source": [
    "**Preprocessing**: $O(n \\cdot d_{\\text{raw}} \\log n)$ for median computation (sorting)\n",
    "\n",
    "**Feature Engineering**: $O(n \\cdot d_{\\text{raw}})$ for element-wise operations\n",
    "\n",
    "**Feature Selection**: $O(n \\cdot d_{\\text{full}} \\cdot c)$ for chi-squared test ($c = 2$ classes)\n",
    "\n",
    "**Single Tree Training**: $O(n \\log n \\cdot m \\cdot \\text{depth})$, where $m = \\sqrt{d} = 3$ features per split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700275e",
   "metadata": {},
   "source": [
    "**Random Forest Training**:\n",
    "\n",
    "$$O_{\\text{RF}} = O(T \\cdot n \\log n \\cdot m \\cdot \\text{depth}) = O(50 \\cdot 164 \\log 164 \\cdot 3 \\cdot 5) \\approx O(600,000)$$\n",
    "\n",
    "**Total Training Complexity**:\n",
    "\n",
    "$$O_{\\text{total}} = O(n d \\log n) + O(T n \\log n \\cdot m \\cdot \\text{depth}) = O(T n \\log n \\cdot m \\cdot \\text{depth})$$\n",
    "\n",
    "**Empirical**: 1.8 seconds on standard CPU\n",
    "\n",
    "The complexity is dominated by tree construction. The small dataset and moderate tree depth enable fast training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba8567",
   "metadata": {},
   "source": [
    "### 7.2 Inference Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc5a2b9",
   "metadata": {},
   "source": [
    "**Preprocessing**: $O(d_{\\text{raw}})$ (constant time for single sample)\n",
    "\n",
    "**Single Tree Prediction**: $O(\\text{depth}) = O(5)$ (traverse root to leaf)\n",
    "\n",
    "**Random Forest Prediction**:\n",
    "\n",
    "$$O_{\\text{inference}} = O(d_{\\text{raw}}) + O(T \\cdot \\text{depth}) = O(50 \\cdot 5) = O(250)$$\n",
    "\n",
    "**Empirical**: <1 millisecond per sample\n",
    "\n",
    "Inference is extremely fast, enabling real-time predictions suitable for web applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d70c6b5",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c6b9d",
   "metadata": {},
   "source": [
    "The NutriSolve ML pipeline is mathematically formulated as a composite function:\n",
    "\n",
    "$$f_{\\text{pipeline}} = h_{\\text{RF}} \\circ \\phi_{\\text{select}} \\circ \\phi_{\\text{encode}} \\circ \\phi_{\\text{engineer}} \\circ \\phi_{\\text{impute}}$$\n",
    "\n",
    "transforming raw 8-dimensional nutritional data through sequential preprocessing stages:\n",
    "\n",
    "1. **Median imputation** of missing values\n",
    "2. **Feature engineering** with 5 derived features (nutrient density, protein ratio, energy density, micronutrient score)\n",
    "3. **One-hot encoding** of 53 food categories\n",
    "4. **Chi-squared feature selection** reducing to 9 dimensions (86.6% reduction)\n",
    "5. **Random Forest classification** with 50 decision trees\n",
    "\n",
    "**Key Parameters**:\n",
    "- Trees: $T = 50$\n",
    "- Max depth: 5\n",
    "- Min samples split: 5\n",
    "- Min samples leaf: 3\n",
    "- Features per split: $m = 3$ (out of 9)\n",
    "\n",
    "**Performance**:\n",
    "- Training accuracy: 93.90%\n",
    "- Test accuracy: 95.12%\n",
    "- F1-Score (macro): 0.9458\n",
    "- ROC-AUC: 0.9815\n",
    "\n",
    "**Complexity**:\n",
    "- Training: $O(600,000)$ operations (~1.8 seconds)\n",
    "- Inference: $O(250)$ operations (<1 millisecond)\n",
    "\n",
    "The model achieves excellent generalization on a small dataset through careful regularization, feature selection, and ensemble methods."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
